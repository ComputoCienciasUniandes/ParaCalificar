{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "import itertools\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primera Parte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Reproduzca la Figura 8.8 del texto guia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Heart.csv')\n",
    "Y = data['AHD'].values.reshape(-1,1).ravel()\n",
    "X = data[['Age', 'Sex', 'RestBP', 'Chol', 'Fbs', 'RestECG', 'MaxHR', 'ExAng', 'Oldpeak', 'Slope']] # quite 'ChestPain', 'Ca', 'Thal' para que me de algo por ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vote(n_Yes_predict, n_No_predict):\n",
    "    # majority votes en cada uno de los subsets\n",
    "    maj_vote = ''\n",
    "    if(n_Yes_predict > n_No_predict):\n",
    "        maj_vote = 'Yes'\n",
    "    elif(n_Yes_predict < n_No_predict):\n",
    "        maj_vote = 'No'\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return maj_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "\n",
    "def Bagging_Error(X, Y, B):\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    kf = sklearn.model_selection.KFold(n_splits=B, shuffle=True)\n",
    "    #num_splits = kf.get_n_splits(X_train) # n_splits = B\n",
    "    \n",
    "    Error_test = []\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # elijo aleatoeriamente B subsets de X_train \n",
    "        x_train, x_test = X.values[train_index, :] , X.values[test_index, :]\n",
    "        y_train, y_test = Y[train_index] , Y[test_index]\n",
    "\n",
    "        # hago predicciones sobre cada x_train\n",
    "        bf = sklearn.ensemble.BaggingClassifier()\n",
    "        bf.fit(x_train, y_train)\n",
    "        Y_predict = bf.predict(x_train)\n",
    "        \n",
    "        score = bf.score(x_test, y_test)\n",
    "        Error_test.append(score)\n",
    "\n",
    "        Yes_predict = (Y_predict == 'Yes')  \n",
    "        No_predict = (Y_predict == 'No')  \n",
    "\n",
    "        # cuento el numero de predicciones en Yes y No\n",
    "        n_Yes_predict = np.count_nonzero(Yes_predict)\n",
    "        n_No_predict = np.count_nonzero(No_predict)\n",
    "\n",
    "        Vote(n_Yes_predict, n_No_predict)\n",
    "\n",
    "    # busco ahora cual es el majority vote de todos los majority votes (me salgo del for)\n",
    "    Yes_predict = (Y_predict == 'Yes')  \n",
    "    No_predict = (Y_predict == 'No')  \n",
    "\n",
    "    # cuento el numero de predicciones en Yes y No\n",
    "    n_Yes_predict = np.count_nonzero(Yes_predict)\n",
    "    n_No_predict = np.count_nonzero(No_predict)\n",
    "\n",
    "    return Vote(n_Yes_predict, n_No_predict), Error_test\n",
    "\n",
    "#print(len(Bagging_Error(X, Y, 5)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "def Random_Forest_Error(X, Y, all_predictors, B):\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # elijo aleatoeriamente B subsets de X_train \n",
    "    kf = sklearn.model_selection.KFold(n_splits=B, shuffle=True)\n",
    "    num_splits = kf.get_n_splits(X_train) # n_splits = B7\n",
    "\n",
    "    # genero una lista de predictores con tamaño m para elegir despues\n",
    "    m_predictors = int(np.sqrt(len(all_predictors)))\n",
    "    combinations = itertools.combinations(all_predictors, m_predictors)\n",
    "\n",
    "    Predictors = []\n",
    "    for c in combinations:\n",
    "        predictors = list(c)\n",
    "        Predictors.append(predictors)\n",
    "\n",
    "    # elijo aleatoriamente los predictores que voy a usar con el tamaño que fije (m), lo hago antes de hacer el split de datos para no tener problemas con el tipopandas frame o nparray\n",
    "    predictors = Predictors[np.random.randint(low=0, high=len(Predictors))]\n",
    "\n",
    "    Error_test = [] # aqui voy a meter el error de la prediccion respecto al test\n",
    "    # Hago el split de los datos en train y test con los parametros elegidos ya modificados \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        x_train, x_test = X[predictors].values[train_index, :] , X[predictors].values[test_index, :]\n",
    "        y_train, y_test = Y[train_index] , Y[test_index]\n",
    "\n",
    "        # hago predicciones sobre cada x_train\n",
    "        bf = sklearn.ensemble.BaggingClassifier()\n",
    "        bf.fit(x_train, y_train)\n",
    "        Y_predict = bf.predict(x_train)\n",
    "\n",
    "        score = bf.score(x_test, y_test)\n",
    "        Error_test.append(score)\n",
    "\n",
    "        Yes_predict = (Y_predict == 'Yes')  \n",
    "        No_predict = (Y_predict == 'No')  \n",
    "\n",
    "        # cuento el numero de predicciones en Yes y No\n",
    "        n_Yes_predict = np.count_nonzero(Yes_predict)\n",
    "        n_No_predict = np.count_nonzero(No_predict)\n",
    "\n",
    "        Vote(n_Yes_predict, n_No_predict)\n",
    "\n",
    "    # busco ahora cual es el majority vote de todos los majority votes (me salgo del for)\n",
    "    Yes_predict = (Y_predict == 'Yes')  \n",
    "    No_predict = (Y_predict == 'No')  \n",
    "\n",
    "    # cuento el numero de predicciones en Yes y No\n",
    "    n_Yes_predict = np.count_nonzero(Yes_predict)\n",
    "    n_No_predict = np.count_nonzero(No_predict)\n",
    "\n",
    "    return Vote(n_Yes_predict, n_No_predict), Error_test\n",
    "\n",
    "\n",
    "all_predictors = ['Age', 'Sex', 'RestBP', 'Chol', 'Fbs', 'RestECG', 'MaxHR', 'ExAng', 'Oldpeak', 'Slope']\n",
    "\n",
    "#print(len(Random_Forest_Error(X,Y, all_predictors, 200)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f0fbb391e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "B = np.arange(0, 200, 1)\n",
    "plt.figure()\n",
    "plt.scatter(B, Random_Forest_Error(X,Y, all_predictors, len(B))[1])\n",
    "plt.scatter(B, Bagging_Error(X,Y, len(B))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Reproduzca la Figura 8.10 del texto guia. Utilice el dataset OJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('OJ.csv')\n",
    "Y = data['Purchase'].values.reshape(-1,1).ravel()\n",
    "X = data[['WeekofPurchase', 'StoreID', 'PriceCH', 'PriceMM', 'DiscCH','DiscMM' , 'SpecialCH',  'SpecialMM',   'LoyalCH', 'SalePriceMM', 'SalePriceCH', 'PriceDiff', 'PctDiscMM',  'PctDiscCH',  'ListPriceDiff',  'STORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(X, Y, all_predictors, m_predictors, B):\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # elijo aleatoeriamente B subsets de X_train \n",
    "    kf = sklearn.model_selection.KFold(n_splits=B, shuffle=True)\n",
    "    num_splits = kf.get_n_splits(X_train) # n_splits = B7\n",
    "\n",
    "    # genero una lista de predictores con tamaño m para elegir despues\n",
    "    combinations = itertools.combinations(all_predictors, m_predictors)\n",
    "\n",
    "    Predictors = []\n",
    "    for c in combinations:\n",
    "        predictors = list(c)\n",
    "        Predictors.append(predictors)\n",
    "\n",
    "    # elijo aleatoriamente los predictores que voy a usar con el tamaño que fije (m), lo hago antes de hacer el split de datos para no tener problemas con el tipopandas frame o nparray\n",
    "    predictors = Predictors[np.random.randint(low=0, high=len(Predictors))]\n",
    "\n",
    "    Error_test = [] # aqui voy a meter el error de la prediccion respecto al test\n",
    "    # Hago el split de los datos en train y test con los parametros elegidos ya modificados \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        x_train, x_test = X[predictors].values[train_index, :] , X[predictors].values[test_index, :]\n",
    "        y_train, y_test = Y[train_index] , Y[test_index]\n",
    "\n",
    "        # hago predicciones sobre cada x_train\n",
    "        bf = sklearn.ensemble.BaggingClassifier()\n",
    "        bf.fit(x_train, y_train)\n",
    "        Y_predict = bf.predict(x_train)\n",
    "\n",
    "        score = bf.score(x_test, y_test)\n",
    "        Error_test.append(score)\n",
    "\n",
    "        Yes_predict = (Y_predict == 'Yes')  \n",
    "        No_predict = (Y_predict == 'No')  \n",
    "\n",
    "        # cuento el numero de predicciones en Yes y No\n",
    "        n_Yes_predict = np.count_nonzero(Yes_predict)\n",
    "        n_No_predict = np.count_nonzero(No_predict)\n",
    "\n",
    "        Vote(n_Yes_predict, n_No_predict)\n",
    "\n",
    "    # busco ahora cual es el majority vote de todos los majority votes (me salgo del for)\n",
    "    Yes_predict = (Y_predict == 'Yes')  \n",
    "    No_predict = (Y_predict == 'No')  \n",
    "\n",
    "    # cuento el numero de predicciones en Yes y No\n",
    "    n_Yes_predict = np.count_nonzero(Yes_predict)\n",
    "    n_No_predict = np.count_nonzero(No_predict)\n",
    "\n",
    "    return Vote(n_Yes_predict, n_No_predict), Error_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_predictors = ['Age', 'Sex', 'RestBP', 'Chol', 'Fbs', 'RestECG', 'MaxHR', 'ExAng', 'Oldpeak', 'Slope']\n",
    "p = len(all_predictors)\n",
    "m_predictors = [p, int(p/2), int(np.sqrt(p))]\n",
    "B = np.arange(0, 200, 1)\n",
    "        \n",
    "plt.figure()\n",
    "#plt.plot(B, Random_Forest_Error(X, Y, all_predictors, m_predictors[0], len(B))[1])\n",
    "#plt.plot(B, Random_Forest_Error(X, Y, all_predictors, m_predictors[1], len(B))[1])\n",
    "#plt.plot(B, Random_Forest(X, Y, all_predictors, m_predictors[2], len(B))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Segunda parte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Utilice el data set OJ para encontrar cual de los siguientes modelos logra predecir mejor el target Purchase:\n",
    "\n",
    "    Logistic regresion.\n",
    "    Linear discriminant analysis.\n",
    "    Classification Tree.\n",
    "    Random Forest. Haga explicito el criterio que utiliza para comparar los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('OJ.csv')\n",
    "Y = data['Purchase'].values.reshape(-1,1).ravel()\n",
    "X = data[['WeekofPurchase', 'StoreID', 'PriceCH', 'PriceMM', 'DiscCH','DiscMM' , 'SpecialCH',  'SpecialMM',   'LoyalCH', 'SalePriceMM', 'SalePriceCH', 'PriceDiff', 'PctDiscMM',  'PctDiscCH',  'ListPriceDiff',  'STORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regresssion \n",
    "import sklearn.linear_model\n",
    "import sklearn.discriminant_analysis\n",
    "\n",
    "f = sklearn.linear_model.LogisticRegression()\n",
    "g = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "\n",
    "def fit(X, Y, f_method, plot=True):\n",
    "    # Split in train and test\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X,Y, test_size=0.4)\n",
    "\n",
    "    #  Fit with train\n",
    "    f_method.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict with test\n",
    "    predict = f_method.predict(X_test)\n",
    "\n",
    "    # This is the probability for the '+' class\n",
    "    proba_yes = f_method.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # loop over n_p different values for the threshold probabiity\n",
    "    n_p = 100\n",
    "    error_rate = np.zeros(n_p)\n",
    "    precision = np.zeros(n_p)\n",
    "    recall = np.zeros(n_p)\n",
    "    tp_rate = np.zeros(n_p)\n",
    "    fp_rate = np.zeros(n_p)\n",
    "    threshold = np.linspace(0,0.99, n_p)\n",
    "\n",
    "    for i in range(n_p):\n",
    "        ii = proba_yes>threshold[i]\n",
    "        predict[ii] = 'CH'\n",
    "        predict[~ii] = 'MM'\n",
    "        true_positive = (Y_test == 'CH') & (predict == 'CH') \n",
    "        false_positive = (Y_test == 'MM') & (predict == 'CH')\n",
    "        false_negative = (Y_test == 'CH') & (predict == 'MM')\n",
    "        true_negative = (Y_test == 'MM') & (predict == 'MM')\n",
    "        n_tp = np.count_nonzero(true_positive)\n",
    "        n_fp= np.count_nonzero(false_positive)\n",
    "        n_fn = np.count_nonzero(false_negative)\n",
    "        n_tn = np.count_nonzero(true_negative)\n",
    "        if((n_tp+n_fp)>0): precision[i] = n_tp/(n_tp + n_fp)\n",
    "        if((n_tp+n_fn)>0): recall[i] = n_tp/(n_tp + n_fn); tp_rate[i] = recall[i]  \n",
    "        if((n_fp+n_tn)>0): fp_rate[i] = n_fp/(n_fp+n_tn)\n",
    "\n",
    "    # compute the area under each curve\n",
    "    ii = np.argsort(recall)\n",
    "    pr_area = scipy.integrate.trapz(precision[ii], recall[ii])\n",
    "    \n",
    "    ii = np.argsort(fp_rate)\n",
    "    roc_area = scipy.integrate.trapz(tp_rate[ii], fp_rate[ii])\n",
    "            \n",
    "    # compute F1\n",
    "    F1 = np.max(2.0*precision*recall/(precision+recall+1E-12))\n",
    "    \n",
    "    return {'pr_area':pr_area, 'roc_area':roc_area, 'F1':F1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification tree\n",
    "import sklearn.tree\n",
    "\n",
    "def fit_ctree(X,Y, B):\n",
    "    for i in range(len(B)):\n",
    "        if(i==0):\n",
    "            pass\n",
    "        else:\n",
    "            h = sklearn.tree.DecisionTreeClassifier(max_depth=int(i))\n",
    "            h.fit(X, Y)\n",
    "\n",
    "            predict = h.predict(X_test)\n",
    "\n",
    "            # This is the probability for the '+' class\n",
    "            proba_yes = h.predict_proba(X_test)[:,1]\n",
    "\n",
    "            # loop over n_p different values for the threshold probabiity\n",
    "            n_p = 100\n",
    "            error_rate = np.zeros(n_p)\n",
    "            precision = np.zeros(n_p)\n",
    "            recall = np.zeros(n_p)\n",
    "            tp_rate = np.zeros(n_p)\n",
    "            fp_rate = np.zeros(n_p)\n",
    "            threshold = np.linspace(0,0.99, n_p)\n",
    "\n",
    "            for i in range(n_p):\n",
    "                ii = proba_yes>threshold[i]\n",
    "                predict[ii] = 'CH'\n",
    "                predict[~ii] = 'MM'\n",
    "                true_positive = (Y_test == 'CH') & (predict == 'CH') \n",
    "                false_positive = (Y_test == 'MM') & (predict == 'CH')\n",
    "                false_negative = (Y_test == 'CH') & (predict == 'MM')\n",
    "                true_negative = (Y_test == 'MM') & (predict == 'MM')\n",
    "                n_tp = np.count_nonzero(true_positive)\n",
    "                n_fp= np.count_nonzero(false_positive)\n",
    "                n_fn = np.count_nonzero(false_negative)\n",
    "                n_tn = np.count_nonzero(true_negative)\n",
    "                if((n_tp+n_fp)>0): precision[i] = n_tp/(n_tp + n_fp)\n",
    "                if((n_tp+n_fn)>0): recall[i] = n_tp/(n_tp + n_fn); tp_rate[i] = recall[i]  \n",
    "                if((n_fp+n_tn)>0): fp_rate[i] = n_fp/(n_fp+n_tn)\n",
    "\n",
    "            # compute the area under each curve\n",
    "            ii = np.argsort(recall)\n",
    "            pr_area = scipy.integrate.trapz(precision[ii], recall[ii])\n",
    "\n",
    "            ii = np.argsort(fp_rate)\n",
    "            roc_area = scipy.integrate.trapz(tp_rate[ii], fp_rate[ii])\n",
    "\n",
    "            # compute F1\n",
    "            F1 = np.max(2.0*precision*recall/(precision+recall+1E-12))\n",
    "    return {'pr_area':pr_area, 'roc_area':roc_area, 'F1':F1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "def fit_RF(X,Y,B):\n",
    "    X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "    for i in range(len(B)):\n",
    "        if(i==0):\n",
    "            pass\n",
    "        else: \n",
    "            j = sklearn.ensemble.RandomForestClassifier(n_estimators=int(i), oob_score=True)\n",
    "            j.fit(X_train, Y_train)\n",
    "            predict = h.predict(X_test)\n",
    "            proba_yes = h.predict_proba(X_test)[:,1]\n",
    "\n",
    "             # loop over n_p different values for the threshold probabiity\n",
    "            n_p = 100\n",
    "            error_rate = np.zeros(n_p)\n",
    "            precision = np.zeros(n_p)\n",
    "            recall = np.zeros(n_p)\n",
    "            tp_rate = np.zeros(n_p)\n",
    "            fp_rate = np.zeros(n_p)\n",
    "            threshold = np.linspace(0,0.99, n_p)\n",
    "\n",
    "            for i in range(n_p):\n",
    "                ii = proba_yes>threshold[i]\n",
    "                predict[ii] = 'CH'\n",
    "                predict[~ii] = 'MM'\n",
    "                true_positive = (Y_test == 'CH') & (predict == 'CH') \n",
    "                false_positive = (Y_test == 'MM') & (predict == 'CH')\n",
    "                false_negative = (Y_test == 'CH') & (predict == 'MM')\n",
    "                true_negative = (Y_test == 'MM') & (predict == 'MM')\n",
    "                n_tp = np.count_nonzero(true_positive)\n",
    "                n_fp= np.count_nonzero(false_positive)\n",
    "                n_fn = np.count_nonzero(false_negative)\n",
    "                n_tn = np.count_nonzero(true_negative)\n",
    "                if((n_tp+n_fp)>0): precision[i] = n_tp/(n_tp + n_fp)\n",
    "                if((n_tp+n_fn)>0): recall[i] = n_tp/(n_tp + n_fn); tp_rate[i] = recall[i]  \n",
    "                if((n_fp+n_tn)>0): fp_rate[i] = n_fp/(n_fp+n_tn)\n",
    "\n",
    "            # compute the area under each curve\n",
    "            ii = np.argsort(recall)\n",
    "            pr_area = scipy.integrate.trapz(precision[ii], recall[ii])\n",
    "\n",
    "            ii = np.argsort(fp_rate)\n",
    "            roc_area = scipy.integrate.trapz(tp_rate[ii], fp_rate[ii])\n",
    "\n",
    "            # compute F1\n",
    "            F1 = np.max(2.0*precision*recall/(precision+recall+1E-12))\n",
    "    return {'pr_area':pr_area, 'roc_area':roc_area, 'F1':F1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el area de la curva de precision-recall es directamente propiorcional a que tan bueno es el modelo, \n",
    "de acuerdo a el siguiente kernel es evidente que para estasituacion los metodos de logisitc regression i discriminant analysis \n",
    "son mejores que classification tree y random forest, para los cuales el area de p-r es muy pequeña\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_area': 0.41670340816286494, 'roc_area': 0.0949879018320083, 'F1': 0.7612156295219598}\n",
      "{'pr_area': 0.4168884902065972, 'roc_area': 0.10596201600294308, 'F1': 0.7594202898546014}\n",
      "{'pr_area': 0.00108038029386344, 'roc_area': 0.001144688644688645, 'F1': 0.04587155963254608}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:460: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/srv/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:465: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_area': 0.00108038029386344, 'roc_area': 0.001144688644688645, 'F1': 0.04587155963254608}\n"
     ]
    }
   ],
   "source": [
    "# Logistic regrsssion\n",
    "print(fit(X, Y, f, plot=True))\n",
    "#lda \n",
    "print(fit(X, Y, g, plot=True))\n",
    "# classification tree\n",
    "B = np.arange(1, 100, 1)\n",
    "print(fit_ctree(X,Y, B))\n",
    "# Random forest\n",
    "print(fit_RF(X,Y, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
